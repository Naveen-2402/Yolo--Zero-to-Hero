{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bbf8fc-e87b-4629-9ea3-3694e9ab0dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from sort import *\n",
    "import cv2\n",
    "import cvzone\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2d22de-0abf-4408-82d6-58ac0b4fbeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classNames = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "              \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
    "              \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\",\n",
    "              \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\",\n",
    "              \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\",\n",
    "              \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\",\n",
    "              \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \"pottedplant\", \"bed\",\n",
    "              \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
    "              \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
    "              \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55824aa-4d01-46e2-be52-609825ccf6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8l.pt')\n",
    "cap = cv2.VideoCapture('Videos/cars.mp4')\n",
    "mask = cv2.imread(\"Images/cars_mask.png\")   # Load an image that acts as a mask, limiting object detection to a specific area (e.g., in which area we need to count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd9a67c-c2ea-415e-a100-bc2a41468f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tracking\n",
    "\n",
    "tracker = Sort(max_age=20, min_hits=3 , iou_threshold=0.3)   # # Initialize the SORT (Simple Online and Realtime Tracking) algorithm for tracking objects from github.\n",
    "\n",
    "# max_age => Number of frames the tracker will keep an object in memory even if itâ€™s not detected (set to 20 frames here).\n",
    "# min_hits => Minimum number of consecutive frames the object must be detected before tracking starts (set to 3 frames here).\n",
    "# iou_threshold => Threshold for Intersection over Union (IoU) used to determine when a detection is considered a match to a tracked object (set to 0.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e125c04-c7eb-4056-ae27-882dacd6ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the objects that cross a specific area (defined by the limits).\n",
    "\n",
    "limits = [400, 297, 673, 297]  # Define the coordinates of a line or region (e.g., x1, y1, x2, y2) that will be used to count objects crossing it.\n",
    "totalCount = []  # List to store the count of objects that have crossed the specified line or region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47973b55-33bc-41ed-8d81-1f35fb6044ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    success, img = cap.read()\n",
    "    imageRegion = cv2.bitwise_and(img,mask)   # Apply the mask to the image using a bitwise AND operation, keeping only the parts of the image that match the mask.\n",
    "    \n",
    "    result = model(imageRegion, stream=True)\n",
    "    detections = np.empty((0,5))    # Initialize an empty NumPy array to store detection results. Each detection will have 5 values: [x_min, y_min, x_max, y_max, confidence_score].\n",
    "    for r in result:\n",
    "        boxes = r.boxes\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "\n",
    "            conf =  (math.ceil(box.conf[0] * 100))/100\n",
    "            cls = int(box.cls[0])\n",
    "            currentClass = classNames[cls]\n",
    "\n",
    "            if currentClass == \"car\" and conf > 0.3:  # Check if the detected object is a \"car\" and the confidence is greater than 0.3.\n",
    "                currentArray = np.array([x1, y1, x2, y2, conf])  # Create an array with the bounding box coordinates (x1, y1, x2, y2) and the confidence score.\n",
    "                detections = np.vstack((detections, currentArray))  # Add the new detection to the detections array by stacking it vertically.\n",
    "\n",
    "    resultsTracker = tracker.update(detections)  # Update the tracker with the current frame's detections. This returns the updated tracking results (e.g., object IDs and their positions).\n",
    "    cv2.line(img, (limits[0], limits[1]), (limits[2], limits[3]), (0, 0, 225), 5)  # Draw a line on the image to define the counting region (using coordinates from 'limits'). The line is red (BGR: 0, 0, 225) and has a thickness of 5 pixels.\n",
    "\n",
    "    \n",
    "    for results in resultsTracker:\n",
    "        x1, y1, x2, y2, Id = results  # Unpack the tracking results: x1, y1, x2, y2 are the bounding box coordinates, and Id is the unique identifier for the tracked object.\n",
    "        x1, y1, x2, y2, Id = int(x1), int(y1), int(x2), int(y2), int(Id)\n",
    "        \n",
    "        print(results)\n",
    "        cvzone.cornerRect(img, (x1, y1, w, h), l=9, rt=2, colorR=(255,0,255))\n",
    "        cvzone.putTextRect(img, f'{Id}' ,(max(0,x1), max(35,y1)), scale = 2, thickness = 3, offset=10)\n",
    "        \n",
    "        cx, cy = int(x1 + w / 2), int(y1 + h / 2)  # Calculate the center coordinates (cx, cy) of the bounding box using its top-left corner (x1, y1) and its width (w) and height (h).\n",
    "        cv2.circle(img, (cx, cy), 5, (255, 0, 255), cv2.FILLED)  # Draw a filled circle at the center (cx, cy) with radius 5, using a magenta color (BGR: 255, 0, 255) on the image.\n",
    "\n",
    "\n",
    "        if limits[0] < cx < limits[2] and limits[1] - 30 < cy < limits[3] + 30:  # Check if the object's center (cx, cy) is within the defined region (with a buffer of 30 pixels vertically).\n",
    "            if Id not in totalCount:  # If the object ID has not been counted yet, add it to the totalCount list to avoid double-counting.\n",
    "                totalCount.append(Id)  # Add the object's ID to the list, indicating it has crossed the counting region.\n",
    "                cv2.line(img, (limits[0], limits[1]), (limits[2], limits[3]), (0, 255, 0), 5)  # Draw a green line (BGR: 0, 255, 0) to visually highlight the counting boundary, with thickness 5 pixels.\n",
    "\n",
    "    cvzone.putTextRect(img, f'Count: {len(totalCount)}' ,(50,50), scale = 2, thickness = 3, offset=10)\n",
    "\n",
    "    \n",
    "    cv2.imshow(\"Video\",img)\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e3415c-1a91-4404-a778-950bd8dd4db7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
